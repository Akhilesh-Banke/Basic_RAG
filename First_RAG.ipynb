{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG-Zw9D5pxvH"
   },
   "source": [
    "#  My First RAG System: From Scratch\n",
    "> A hands-on Retrieval Augmented Generation (RAG) project built using only open-source tools.\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "In this notebook, I'll build my own **RAG (Retrieval Augmented Generation)** system step-by-step.\n",
    "\n",
    " Goal: Make LLMs stop hallucinating by grounding them in real data — my data!\n",
    "\n",
    "I'll cover:\n",
    "1.  Loading custom knowledge\n",
    "2.  Chunking it smartly\n",
    "3.  Embedding with Sentence Transformers\n",
    "4.  Storing & retrieving with FAISS\n",
    "5.  Generating grounded answers\n",
    "\n",
    "---\n",
    "\n",
    "**Libraries used:**\n",
    "transformers – for our LLM\n",
    "sentence-transformers – for embeddings\n",
    "faiss-cpu – for vector search\n",
    "langchain – for text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers sentence-transformers faiss-cpu langchain torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JETQi12MqkVx"
   },
   "source": [
    "### **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCu6NxXg3J15"
   },
   "source": [
    "### **Our Data**\n",
    "First, we need some custom knowledge. Let’s import the my_knwledge.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knowledge base saved.\n"
     ]
    }
   ],
   "source": [
    "# Load our document\n",
    "with open(\"/content/my_knwledge.txt\") as f:\n",
    "    knowledge_text = f.read()\n",
    "\n",
    "print(\"✅ Knowledge base saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Company Policy Manual:\\n- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays and Fridays are optional remote days.\\n- PTO Policy: Full-time employees receive 20 days of Paid Time Off (PTO) per year. PTO accrues monthly.\\n- Tech Stack: The official backend language is Python, and the official frontend framework is React. For mobile development, we use React Native.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpBfSZqQ2v4H"
   },
   "source": [
    "### **Chunking**\n",
    "We can’t feed the whole book to the model at once. We need to split it into index cards (chunks). Don’t just split by \\n (newlines). We’ll cut sentences in half. We’ll use a smart splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5 chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Company Policy Manual:\n",
      "\n",
      "--- Chunk 2 ---\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Thursdays. Mondays and Fridays are optional remote days.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "- PTO Policy: Full-time employees receive 20 days of Paid Time Off (PTO) per year. PTO accrues monthly.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "- Tech Stack: The official backend language is Python, and the official frontend framework is React. For mobile development, we use React Native.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" This splitter is smart. It tries to split on paragraphs (\"\\n\\n\"),\n",
    " then newlines (\"\\n\"), then spaces (\" \"), to keep semantically\n",
    " related text together as much as possible. \"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split into chunks\n",
    "chunks = text_splitter.split_text(knowledge_text)\n",
    "\n",
    "print(f\"We have {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM2PRcVH5cFV"
   },
   "source": [
    "Yeah..! it intelligently broke our file into small, overlapping pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTRlbifE5jwl"
   },
   "source": [
    "### **Embeddings**\n",
    "Now we turn those text chunks into numbers (vectors). We’ll use a popular, lightweight sentence-transformer model. It’s brilliant at understanding the meaning of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7ed72df0fd43f0b5a4e0a3147c098b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c04b8f18b0b417f9068bf53157f794d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71a0046790b4e7ab0081f216dca667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e7cba6bd544a5a9a5ed95214421b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ebe5e9fd2d46dbbbd4f17dc7a2e9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0187ec970a9a4d8e9d7d8dadee1cc678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06418192f530439e8ba359ef3be13b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5db84fd9e30424484a91fba865970f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80211d084c7341d3ba21762487d0d48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066d56a2bf6c4553a41cdcc7a9889098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd43be8332842fc80bfcf7e97b035e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our embeddings: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the embedding model\n",
    "# 'all-MiniLM-L6-v2' is a fantastic, fast, and small model.\n",
    "# It runs 100% on your local machine.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed all our chunks\n",
    "# This will take a moment as it \"reads\" and \"understands\" each chunk.\n",
    "chunk_embeddings = model.encode(chunks)\n",
    "\n",
    "print(f\"Shape of our embeddings: {chunk_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOqvHUWe6FmL"
   },
   "source": [
    "### **Vector Store with FAISS**\n",
    "We have our vectors. Now we need a database to store them in a way we can search by similarity. It is where FAISS comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 5 vectors.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Get the dimension of our vectors (e.g., 384)\n",
    "d = chunk_embeddings.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "# IndexFlatL2 is the simplest, most basic index. It calculates\n",
    "# the exact distance (L2 distance) between our query and all vectors.\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add our chunk embeddings to the index\n",
    "# We must convert to float32 for FAISS\n",
    "index.add(np.array(chunk_embeddings).astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QojRU8eL6gz7"
   },
   "source": [
    "That’s it. We just created an in-memory vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fL0CrB26pMB"
   },
   "source": [
    "### **Retrieve, Augment, Generate**\n",
    "This is the final part. Here the user will ask a question. Let’s trace the full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b772a93d89204b0ab004466b7fa2160b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d819c5da4b3e4f2f9003f9e857f556ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b27b6a01af46218a3559029a01ea86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99b093653b84ec19b02b6174121854f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d670b9a0fc184e8aa0d2294a2a958a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600ae3bb78a14d10b5de0ae840a99f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b069b8c0824f61b57281978ed5b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. Load a \"Question-Answering\" or \"Text-Generation\" model\n",
    "# We'll use a small, instruction-tuned model from Google.\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-small')\n",
    "\n",
    "# --- This is our RAG pipeline function ---\n",
    "def answer_question(query):\n",
    "    # 1. RETRIEVE\n",
    "    # Embed the user's query\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "    # Search the FAISS index for the top k (e.g., k=2) most similar chunks\n",
    "    k = 2\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Get the actual text chunks from our original 'chunks' list\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # 2. AUGMENT\n",
    "    # This is the \"magic prompt.\" We combine the retrieved context\n",
    "    # with the user's query.\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the following question using *only* the provided context.\n",
    "    If the answer is not in the context, say \"I don't have that information.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. GENERATE\n",
    "    # Feed the augmented prompt to our generative model\n",
    "    answer = generator(prompt_template, max_length=100)\n",
    "    print(f\"--- CONTEXT ---\\n{context}\\n\")\n",
    "    return answer[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "719RqAMV9fFq"
   },
   "source": [
    "Now, let’s ask our system some questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the WFH policy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONTEXT ---\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "Company Policy Manual:\n",
      "\n",
      "Answer: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays Company Policy Manual:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"What is the WFH policy?\"\n",
    "print(f\"Query: {query_1}\")\n",
    "print(f\"Answer: {answer_question(query_1)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pKTGq_F9tIe"
   },
   "source": [
    "It worked! It didn’t just guess, it found the exact text and synthesized the answer.\n",
    "\n",
    "Now, let’s ask a question the context cannot answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the company's dental plan?\n",
      "--- CONTEXT ---\n",
      "Company Policy Manual:\n",
      "\n",
      "- WFH Policy: All employees are eligible for a hybrid WFH schedule. Employees must be in the office on Tuesdays, Wednesdays, and Thursdays. Mondays\n",
      "\n",
      "Answer: I don't have that information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"What is the company's dental plan?\"\n",
    "print(f\"Query: {query_2}\")\n",
    "print(f\"Answer: {answer_question(query_2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyI0ehmj-Hnq"
   },
   "source": [
    "It is critical. Because of our prompt (“only use the provided context”), the LLM didn’t hallucinate. It correctly stated it couldn’t find the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK0KtTof-bv6"
   },
   "source": [
    "### **Summary:**\n",
    "Take a step back. What we just built in a few dozen lines of Python is the foundation of the next generation of AI. We solved the three biggest problems with LLMs:\n",
    "\n",
    "\n",
    "Hallucinations: We grounded the model in reality.\n",
    "Stale Knowledge: We can update the knowledge! Just we've to re-run the indexing on new documents.\n",
    "Data Privacy: No data ever left our computer. The embedding model and the LLM all ran locally."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
